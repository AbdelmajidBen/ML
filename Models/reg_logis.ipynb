{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.6865\n",
      "Epoch 20, Loss: 0.6802\n",
      "Epoch 30, Loss: 0.6742\n",
      "Epoch 40, Loss: 0.6684\n",
      "Epoch 50, Loss: 0.6629\n",
      "Epoch 60, Loss: 0.6576\n",
      "Epoch 70, Loss: 0.6526\n",
      "Epoch 80, Loss: 0.6478\n",
      "Epoch 90, Loss: 0.6432\n",
      "Epoch 100, Loss: 0.6388\n",
      "Epoch 110, Loss: 0.6346\n",
      "Epoch 120, Loss: 0.6306\n",
      "Epoch 130, Loss: 0.6267\n",
      "Epoch 140, Loss: 0.6230\n",
      "Epoch 150, Loss: 0.6195\n",
      "Epoch 160, Loss: 0.6160\n",
      "Epoch 170, Loss: 0.6128\n",
      "Epoch 180, Loss: 0.6096\n",
      "Epoch 190, Loss: 0.6066\n",
      "Epoch 200, Loss: 0.6037\n",
      "Epoch 210, Loss: 0.6009\n",
      "Epoch 220, Loss: 0.5982\n",
      "Epoch 230, Loss: 0.5956\n",
      "Epoch 240, Loss: 0.5931\n",
      "Epoch 250, Loss: 0.5907\n",
      "Epoch 260, Loss: 0.5884\n",
      "Epoch 270, Loss: 0.5861\n",
      "Epoch 280, Loss: 0.5840\n",
      "Epoch 290, Loss: 0.5819\n",
      "Epoch 300, Loss: 0.5798\n",
      "Epoch 310, Loss: 0.5779\n",
      "Epoch 320, Loss: 0.5760\n",
      "Epoch 330, Loss: 0.5741\n",
      "Epoch 340, Loss: 0.5724\n",
      "Epoch 350, Loss: 0.5706\n",
      "Epoch 360, Loss: 0.5690\n",
      "Epoch 370, Loss: 0.5673\n",
      "Epoch 380, Loss: 0.5657\n",
      "Epoch 390, Loss: 0.5642\n",
      "Epoch 400, Loss: 0.5627\n",
      "Epoch 410, Loss: 0.5612\n",
      "Epoch 420, Loss: 0.5598\n",
      "Epoch 430, Loss: 0.5584\n",
      "Epoch 440, Loss: 0.5571\n",
      "Epoch 450, Loss: 0.5558\n",
      "Epoch 460, Loss: 0.5545\n",
      "Epoch 470, Loss: 0.5533\n",
      "Epoch 480, Loss: 0.5520\n",
      "Epoch 490, Loss: 0.5508\n",
      "Epoch 500, Loss: 0.5497\n",
      "Epoch 510, Loss: 0.5485\n",
      "Epoch 520, Loss: 0.5474\n",
      "Epoch 530, Loss: 0.5463\n",
      "Epoch 540, Loss: 0.5453\n",
      "Epoch 550, Loss: 0.5442\n",
      "Epoch 560, Loss: 0.5432\n",
      "Epoch 570, Loss: 0.5422\n",
      "Epoch 580, Loss: 0.5412\n",
      "Epoch 590, Loss: 0.5402\n",
      "Epoch 600, Loss: 0.5393\n",
      "Epoch 610, Loss: 0.5383\n",
      "Epoch 620, Loss: 0.5374\n",
      "Epoch 630, Loss: 0.5365\n",
      "Epoch 640, Loss: 0.5356\n",
      "Epoch 650, Loss: 0.5347\n",
      "Epoch 660, Loss: 0.5339\n",
      "Epoch 670, Loss: 0.5330\n",
      "Epoch 680, Loss: 0.5322\n",
      "Epoch 690, Loss: 0.5314\n",
      "Epoch 700, Loss: 0.5306\n",
      "Epoch 710, Loss: 0.5298\n",
      "Epoch 720, Loss: 0.5290\n",
      "Epoch 730, Loss: 0.5282\n",
      "Epoch 740, Loss: 0.5274\n",
      "Epoch 750, Loss: 0.5267\n",
      "Epoch 760, Loss: 0.5259\n",
      "Epoch 770, Loss: 0.5252\n",
      "Epoch 780, Loss: 0.5244\n",
      "Epoch 790, Loss: 0.5237\n",
      "Epoch 800, Loss: 0.5230\n",
      "Epoch 810, Loss: 0.5223\n",
      "Epoch 820, Loss: 0.5216\n",
      "Epoch 830, Loss: 0.5209\n",
      "Epoch 840, Loss: 0.5202\n",
      "Epoch 850, Loss: 0.5196\n",
      "Epoch 860, Loss: 0.5189\n",
      "Epoch 870, Loss: 0.5182\n",
      "Epoch 880, Loss: 0.5176\n",
      "Epoch 890, Loss: 0.5169\n",
      "Epoch 900, Loss: 0.5163\n",
      "Epoch 910, Loss: 0.5156\n",
      "Epoch 920, Loss: 0.5150\n",
      "Epoch 930, Loss: 0.5144\n",
      "Epoch 940, Loss: 0.5137\n",
      "Epoch 950, Loss: 0.5131\n",
      "Epoch 960, Loss: 0.5125\n",
      "Epoch 970, Loss: 0.5119\n",
      "Epoch 980, Loss: 0.5113\n",
      "Epoch 990, Loss: 0.5107\n",
      "Epoch 1000, Loss: 0.5101\n",
      "Final Coefficients (beta_0, beta_1, beta_2): [-0.70428759  0.48828296 -0.40549518]\n",
      "Predictions: [0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Logistic Regression using Gradient Descent\n",
    "def logistic_regression(X, y, learning_rate=0.001, epochs=100):\n",
    "    # Initialize coefficients (beta_0, beta_1, beta_2)\n",
    "    n_samples, n_features = X.shape\n",
    "    beta = np.zeros(n_features)  # beta[0] is beta_0, beta[1] is beta_1, etc.\n",
    "\n",
    "    # Gradient Descent\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(n_samples):\n",
    "            xi = X[i]  # i-th sample features (X1, X2, ..., Xn)\n",
    "            yi = y[i]  # i-th sample label (Y)\n",
    "\n",
    "            # Predict the output for the current sample\n",
    "            z = np.dot(beta, xi)\n",
    "            y_hat = sigmoid(z)\n",
    "            \n",
    "            # Calculate error\n",
    "            error = y_hat - yi\n",
    "            \n",
    "            # Update the coefficients (weights)\n",
    "            beta -= learning_rate * error * xi\n",
    "\n",
    "        # Optionally print the progress\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            loss = -np.mean(y * np.log(sigmoid(np.dot(X, beta))) + (1 - y) * np.log(1 - sigmoid(np.dot(X, beta))))\n",
    "            print(f'Epoch {epoch+1}, Loss: {loss:.4f}')\n",
    "    \n",
    "    return beta\n",
    "\n",
    "# Example data (X1, X2)\n",
    "X = np.array([[2.7810836, 2.550537003],\n",
    "              [1.465489372, 2.362125076],\n",
    "              [3.396561688, 4.400293529],\n",
    "              [1.38807019, 1.850220317],\n",
    "              [3.06407232, 3.005305973],\n",
    "              [7.627531214, 2.759262235]])\n",
    "\n",
    "# Labels (Y)\n",
    "y = np.array([0, 0, 1, 0, 0, 1])\n",
    "\n",
    "# Add the intercept (bias term) to the feature matrix X\n",
    "# This creates a new column with all ones for beta_0\n",
    "X_with_bias = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "# Train the logistic regression model\n",
    "learning_rate = 0.001\n",
    "epochs = 1000\n",
    "beta = logistic_regression(X_with_bias, y, learning_rate, epochs)\n",
    "\n",
    "# Final coefficients\n",
    "print(\"Final Coefficients (beta_0, beta_1, beta_2):\", beta)\n",
    "\n",
    "# Predict using the final coefficients\n",
    "def predict(X, beta):\n",
    "    z = np.dot(X, beta)\n",
    "    return sigmoid(z)\n",
    "\n",
    "# Test predictions\n",
    "predictions = predict(X_with_bias, beta)\n",
    "predicted_classes = [1 if p >= 0.5 else 0 for p in predictions]\n",
    "print(\"Predictions:\", predicted_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
